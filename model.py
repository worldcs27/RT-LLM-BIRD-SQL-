"""
BIRD-SQL ç‰ˆæœ¬çš„ DeepSeek Relational Transformer (æ”¯æŒ CoT ç”Ÿæˆ)
- ç§»é™¤äº† RelBench ä¾èµ–
- ç§»é™¤äº†åˆ†ç±»å¤´ï¼Œæ”¹ä¸º Causal LM ç”Ÿæˆå¤´
- å¢åŠ äº† generate() æ–¹æ³•æ”¯æŒæ€ç»´é“¾ (CoT)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer
from torch_frame import stype
from task_type import TaskType  # ä½¿ç”¨è‡ªå®šä¹‰ TaskType
import types

# é»˜è®¤ä½¿ç”¨çš„æ–‡æœ¬ç¼–ç ç»´åº¦ (DeepSeek-R1-Distill-Qwen-1.5B ä¸º 1536, MiniLM ä¸º 384, 7B/16B å¯èƒ½æ˜¯ 2048/4096)
# è¯·æ ¹æ® bird_adapter.py ä¸­ä½¿ç”¨çš„æ¨¡å‹è°ƒæ•´æ­¤å€¼
DEFAULT_TEXT_EMBED_DIM = 1536 

# --- RT Blocks (ä¿æŒä¸å˜) ---
class RelationalTransformerBlock(nn.Module):
    """å…³ç³»å‹ Transformer Blockï¼ŒåŒ…å«å››ç§æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.norms = nn.ModuleDict({
            "col": nn.LayerNorm(embed_dim),
            "feat": nn.LayerNorm(embed_dim),
            "nbr": nn.LayerNorm(embed_dim),
            "full": nn.LayerNorm(embed_dim),
            "ffn": nn.LayerNorm(embed_dim)
        })
        self.attns = nn.ModuleDict({
            "col": nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout),
            "feat": nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout),
            "nbr": nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout),
            "full": nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout)
        })
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.SiLU(), # ä½¿ç”¨ SiLU æ¿€æ´»
            nn.Dropout(dropout),
            nn.Linear(4 * embed_dim, embed_dim)
        )

    def forward(self, x, block_masks):
        """å‰å‘ä¼ æ’­ï¼Œåº”ç”¨å››ç§æ³¨æ„åŠ›æœºåˆ¶"""
        for l in ["col", "feat", "nbr", "full"]:
            residual = x
            norm_x = self.norms[l](x)
            mask_bool = block_masks[l]
            attn_mask = None
            if l != "full":
                attn_mask = ~mask_bool
            attn_out, _ = self.attns[l](
                query=norm_x, key=norm_x, value=norm_x,
                attn_mask=attn_mask, need_weights=False
            )
            x = residual + attn_out
        x = x + self.ffn(self.norms["ffn"](x))
        return x

class RTEmbedding(nn.Module):
    """å…³ç³»å‹ Transformer åµŒå…¥å±‚ï¼Œå¤„ç†è¡¨ã€åˆ—ã€å€¼çš„åµŒå…¥"""
    def __init__(self, channels, node_to_col_names, node_to_col_stats, table_list, text_embed_dim=DEFAULT_TEXT_EMBED_DIM):
        super().__init__()
        self.channels = channels
        self.node_to_col_names = node_to_col_names
        self.table_list = table_list
        self.table_to_idx = {t: i for i, t in enumerate(table_list)}
        
        self.table_emb = nn.Embedding(len(table_list), channels)
        self.col_embs = nn.ModuleDict()
        self.val_encoders = nn.ModuleDict()
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†è‡ªåŠ¨åŠ è½½ SentenceTransformerï¼Œè€Œæ˜¯å‡è®¾ bird_adapter å·²ç»ä¼ å¥½äº† embedding tensor
        # text_embed_dim å¿…é¡»ä¸ bird_adapter.py ä¸­ä½¿ç”¨çš„æ¨¡å‹è¾“å‡ºç»´åº¦ä¸€è‡´
            
        for table in table_list:
            if table not in node_to_col_names: continue
                
            col_names = node_to_col_names[table]
            stats = node_to_col_stats.get(table, {})
            t_val_encs = nn.ModuleDict()
            t_col_embs = nn.ParameterDict()
            
            # å¤„ç†æ•°å€¼åˆ—
            for col in col_names.get(stype.numerical, []):
                t_val_encs[col] = nn.Sequential(nn.Linear(1, channels), nn.SiLU())
                t_col_embs[col] = nn.Parameter(torch.randn(channels))
            
            # å¤„ç†åˆ†ç±»åˆ—
            for col in col_names.get(stype.categorical, []):
                # ... (ä¿æŒåŸæœ‰çš„ categorical ç»Ÿè®¡é€»è¾‘) ...
                num_cats = 100 # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯»å– stats
                if isinstance(stats, dict) and col in stats and stype.categorical in stats[col]:
                     if 'vocab' in stats[col][stype.categorical]:
                         num_cats = len(stats[col][stype.categorical]['vocab'])
                
                t_val_encs[col] = nn.Embedding(num_cats + 1, channels)
                t_col_embs[col] = nn.Parameter(torch.randn(channels))
            
            # å¤„ç†æ–‡æœ¬åµŒå…¥åˆ— (è¿™æ˜¯ BIRD-SQL çš„é‡ç‚¹)
            for col in col_names.get(stype.text_embedded, []):
                # å…³é”®ä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦æ”¹ä¸º text_embed_dim
                t_val_encs[col] = nn.Linear(text_embed_dim, channels) 
                t_col_embs[col] = nn.Parameter(torch.randn(channels))
                
            self.val_encoders[table] = t_val_encs
            self.col_embs[table] = t_col_embs

    def forward(self, tf_dict, edge_index_dict=None, table_to_node_offset=None):
        """å‰å‘ä¼ æ’­ï¼Œå°† TensorFrame è½¬æ¢ä¸ºåµŒå…¥åºåˆ—"""
        all_embs = []
        node_idxs_list = []
        table_idxs_list = []
        curr_node_offset = 0
        
        for table_name in self.table_list:
            if table_name not in tf_dict: continue
            tf = tf_dict[table_name]
            num_rows = tf.num_rows
            if num_rows == 0: continue
            
            t_idx = self.table_to_idx[table_name]
            t_emb = self.table_emb(torch.tensor(t_idx, device=tf.device))
            
            def add_token(val_enc, col_enc, val_data):
                token = val_enc(val_data) + col_enc + t_emb
                all_embs.append(token)
                node_idxs_list.append(torch.arange(curr_node_offset, curr_node_offset+num_rows, device=tf.device))
                table_idxs_list.append(torch.full((num_rows,), t_idx, device=tf.device))

            col_names = self.node_to_col_names[table_name]
            
            # å¤„ç†å„ç±»å‹åˆ—
            if stype.numerical in tf.feat_dict:
                feat = tf.feat_dict[stype.numerical]
                for i, col in enumerate(col_names.get(stype.numerical, [])):
                    add_token(self.val_encoders[table_name][col], self.col_embs[table_name][col], feat[:, i:i+1])
            
            if stype.categorical in tf.feat_dict:
                feat = tf.feat_dict[stype.categorical]
                for i, col in enumerate(col_names.get(stype.categorical, [])):
                    add_token(self.val_encoders[table_name][col], self.col_embs[table_name][col], feat[:, i])
            
            if stype.text_embedded in tf.feat_dict:
                feat = tf.feat_dict[stype.text_embedded]
                for i, col in enumerate(col_names.get(stype.text_embedded, [])):
                    # feat[:, i, :] åº”è¯¥æ˜¯ [Num_Rows, text_embed_dim]
                    add_token(self.val_encoders[table_name][col], self.col_embs[table_name][col], feat[:, i, :])
            
            curr_node_offset += num_rows
            
        if not all_embs: return None, None, None, None, None, 0
        
        x = torch.cat(all_embs, dim=0)
        node_idxs = torch.cat(node_idxs_list, dim=0)
        table_idxs = torch.cat(table_idxs_list, dim=0)
        
        # Col Idxs reconstruction
        col_idxs_tensor_list = []
        c_counter = 0
        for table_name in self.table_list:
            if table_name not in tf_dict: continue
            num = tf_dict[table_name].num_rows
            if num == 0: continue
            cnt = len(self.node_to_col_names[table_name].get(stype.numerical, [])) + \
                  len(self.node_to_col_names[table_name].get(stype.categorical, [])) + \
                  len(self.node_to_col_names[table_name].get(stype.text_embedded, []))
            for _ in range(cnt):
                col_idxs_tensor_list.append(torch.full((num,), c_counter, device=x.device))
                c_counter += 1
        col_idxs = torch.cat(col_idxs_tensor_list, dim=0)
        
        # æ„å»º f2p_nbr_idxs (ç®€åŒ–ç‰ˆï¼Œå¤ç”¨ä¹‹å‰é€»è¾‘)
        f2p_nbr_idxs = None
        if edge_index_dict and table_to_node_offset:
            max_parents = 16
            S = node_idxs.shape[0]
            device = x.device
            f2p_nbr_idxs = torch.full((S, max_parents), -1, dtype=torch.long, device=device)
            f2p_counts = torch.zeros(S, dtype=torch.long, device=device)
            
            node_to_cells = {}
            for cell_idx in range(S):
                node_idx = node_idxs[cell_idx].item()
                if node_idx not in node_to_cells: node_to_cells[node_idx] = []
                node_to_cells[node_idx].append(cell_idx)
            
            for edge_type, edge_index in edge_index_dict.items():
                src_table_name, rel_name, dst_table_name = edge_type
                if "rev_" in rel_name or "rev_fkey" in rel_name: continue
                if edge_index.shape[1] == 0: continue
                
                src_offset = table_to_node_offset.get(src_table_name, 0)
                dst_offset = table_to_node_offset.get(dst_table_name, 0)
                child_nodes_global = edge_index[0] + src_offset
                parent_nodes_global = edge_index[1] + dst_offset
                
                for i in range(edge_index.shape[1]):
                    child_node_global = child_nodes_global[i].item()
                    parent_node_global = parent_nodes_global[i].item()
                    if child_node_global not in node_to_cells: continue
                    for c_cell in node_to_cells[child_node_global]:
                        count = f2p_counts[c_cell].item()
                        if count < max_parents:
                            f2p_nbr_idxs[c_cell, count] = parent_node_global
                            f2p_counts[c_cell] += 1
        
        return x, node_idxs, col_idxs, table_idxs, f2p_nbr_idxs, curr_node_offset

# --- DeepSeek MoE Forward Fix (ä¿æŒä¸å˜) ---
def deepseek_moe_forward_fixed(self, hidden_states):
    identity = hidden_states
    orig_shape = hidden_states.shape
    topk_idx, topk_weight, aux_loss = self.gate(hidden_states)
    hidden_states = hidden_states.view(-1, hidden_states.shape[-1])
    flat_topk_idx = topk_idx.view(-1)
    if self.num_experts_per_tok > 1:
        hidden_states = hidden_states.repeat_interleave(self.num_experts_per_tok, dim=0)
    y = torch.empty_like(hidden_states, dtype=hidden_states.dtype)
    for i, expert in enumerate(self.experts):
        idx_mask = (flat_topk_idx == i)
        if idx_mask.any():
            expert_out = expert(hidden_states[idx_mask])
            y[idx_mask] = expert_out.to(y.dtype)
    y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
    y = y.to(hidden_states.dtype).view(*orig_shape)
    y = y + identity
    return y

class DeepSeekRelationalModel(nn.Module):
    """DeepSeek å…³ç³»å‹ Transformer ä¸»æ¨¡å‹ (Generative / CoT Enabled)"""
    def __init__(self, data, col_stats_dict, args, task=None):
        super().__init__()
        self.task = task
        self.model_type = args.model_type
        self.main_device = torch.device("cuda:0") 
        self.hidden_dim = args.channels
        
        # ä¿å­˜åŸå§‹ data
        self.original_data = data
        
        col_names_dict = {}
        valid_table_list = []
        for node_type in data.node_types:
            if hasattr(data[node_type], 'tf') and data[node_type].tf is not None:
                col_names_dict[node_type] = data[node_type].tf.col_names_dict
                valid_table_list.append(node_type)
        
        filtered_col_stats_dict = {t: col_stats_dict.get(t, {}) for t in valid_table_list}
        
        # è·å– Embedding ç»´åº¦ï¼Œä¼˜å…ˆä» args è·å–ï¼Œå¦åˆ™ç”¨é»˜è®¤
        text_embed_dim = getattr(args, 'text_embed_dim', DEFAULT_TEXT_EMBED_DIM)

        # RT Tokenizer & Layers
        self.tokenizer = RTEmbedding(
            channels=self.hidden_dim,
            node_to_col_names=col_names_dict,
            node_to_col_stats=filtered_col_stats_dict,
            table_list=valid_table_list,
            text_embed_dim=text_embed_dim
        )
        self.rt_layers = nn.ModuleList([
            RelationalTransformerBlock(self.hidden_dim, num_heads=4, dropout=args.dropout)
            for _ in range(args.num_layers)
        ])

        # LLM Loading
        print(f"ğŸš€ Loading LLM ({self.model_type}) for Generation...")
        self.llm = AutoModelForCausalLM.from_pretrained(
            self.model_type, 
            device_map="auto",
            torch_dtype=torch.float16, 
            trust_remote_code=True
        )
        # åŠ è½½ Tokenizer (ç”¨äº generate æ–¹æ³•)
        self.llm_tokenizer = AutoTokenizer.from_pretrained(self.model_type, trust_remote_code=True)
        self.llm_tokenizer.padding_side = 'left' # è¿™ä¸€æ­¥å¾ˆå…³é”®
        if self.llm_tokenizer.pad_token is None:
             self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

        # ä¿®å¤ MoE Forward
        for module in self.llm.modules():
            if hasattr(module, 'gate') and hasattr(module, 'experts'):
                 module.forward = types.MethodType(deepseek_moe_forward_fixed, module)
        
        # å†»ç»“ LLM å‚æ•°
        for param in self.llm.parameters(): param.requires_grad = False
            
        # Projector: RT Dim -> LLM Dim
        llm_dim = self.llm.config.hidden_size
        self.projector = nn.Sequential(
            nn.Linear(self.hidden_dim, 1024),
            nn.SiLU(), 
            nn.Linear(1024, llm_dim)
        )
        
        # Attention-based Pooling for structure aggregation
        # ç”¨äºæ›¿ä»£ç®€å•çš„ Mean Poolingï¼Œæ›´å¥½åœ°èšåˆé‡è¦èŠ‚ç‚¹
        self.attention_pool = nn.MultiheadAttention(
            embed_dim=self.hidden_dim,
            num_heads=4,
            batch_first=True,
            dropout=args.dropout
        )
        self.pool_query = nn.Parameter(torch.randn(1, 1, self.hidden_dim))
        
        # ç§»é™¤äº† self.head (ä¸å†éœ€è¦)

        # ç§»åŠ¨åˆ°ä¸»è®¾å¤‡
        self.tokenizer.to(self.main_device)
        self.rt_layers.to(self.main_device)
        self.projector.to(self.main_device)
        self.attention_pool.to(self.main_device)
        # pool_query å·²ç»æ˜¯ Parameterï¼Œç›´æ¥ç§»åŠ¨åˆ°è®¾å¤‡
        self.pool_query.data = self.pool_query.data.to(self.main_device)

    def create_masks(self, node_idxs, col_idxs, table_idxs, f2p_nbr_idxs=None, is_padding=None):
        """(ä¿æŒåŸæ ·) æ„å»ºå››ç§æ³¨æ„åŠ›æœºåˆ¶çš„mask"""
        # ... (Create Masks ä»£ç ä¸ä¹‹å‰ä¸€è‡´ï¼Œçœç•¥ä»¥èŠ‚çœç©ºé—´ï¼Œè¯·ä¿ç•™åŸæœ‰çš„å®ç°) ...
        # è¯·åŠ¡å¿…ä¿ç•™åŸæ–‡ä»¶ä¸­çš„ create_masks å®ç°ï¼Œä¸è¦åˆ é™¤ï¼
        if node_idxs.dim() == 1:
            node_idxs = node_idxs.unsqueeze(0)
            col_idxs = col_idxs.unsqueeze(0)
            table_idxs = table_idxs.unsqueeze(0)
            if f2p_nbr_idxs is not None and f2p_nbr_idxs.dim() == 2:
                f2p_nbr_idxs = f2p_nbr_idxs.unsqueeze(0)
            if is_padding is not None and is_padding.dim() == 1:
                is_padding = is_padding.unsqueeze(0)
        B, S = node_idxs.shape
        if is_padding is not None:
            pad = (~is_padding[:, :, None]) & (~is_padding[:, None, :])
        else:
            pad = torch.ones((B, S, S), dtype=torch.bool, device=node_idxs.device)
        same_node = node_idxs[:, :, None] == node_idxs[:, None, :]
        same_col = col_idxs[:, :, None] == col_idxs[:, None, :]
        same_tab = table_idxs[:, :, None] == table_idxs[:, None, :]
        same_col_table = same_col & same_tab
        if f2p_nbr_idxs is not None:
            kv_in_f2p = (node_idxs[:, None, :, None] == f2p_nbr_idxs[:, :, None, :]).any(-1)
            q_in_f2p = (node_idxs[:, :, None, None] == f2p_nbr_idxs[:, None, :, :]).any(-1)
        else:
            kv_in_f2p = torch.zeros((B, S, S), dtype=torch.bool, device=node_idxs.device)
            q_in_f2p = torch.zeros((B, S, S), dtype=torch.bool, device=node_idxs.device)
        masks = {
            "feat": (same_node | kv_in_f2p) & pad,
            "nbr": q_in_f2p & pad,
            "col": same_col_table & pad,
            "full": pad
        }
        if B == 1:
            for key in masks: masks[key] = masks[key].squeeze(0)
        return masks

    def encode_structure(self, batch):
        """
        æå– RT ç»“æ„ç‰¹å¾ (æ ¸å¿ƒé€»è¾‘å¤ç”¨)
        Returns: 
            x_llm: [1, LLM_Dim] (Projected RT output) æˆ– None (å¦‚æœå‡ºé”™)
        """
        try:
            # 1. å‡†å¤‡ TF Dict
            tf_dict = {}
            for node_type in batch.node_types:
                if hasattr(batch[node_type], 'tf') and batch[node_type].tf is not None:
                    tf_dict[node_type] = batch[node_type].tf
            
            # Fallback to original data if needed
            for node_type in batch.node_types:
                if node_type not in tf_dict and node_type in self.original_data.node_types:
                    original_tf = getattr(self.original_data[node_type], 'tf', None)
                    if original_tf is not None:
                        # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”æ ¹æ®ç´¢å¼•åˆ‡ç‰‡
                        tf_dict[node_type] = original_tf
                        
            if not tf_dict:
                print("âš ï¸ Warning: No TensorFrame found in batch")
                return None

            # 2. è®¡ç®— Offset
            table_to_node_offset = {}
            node_offset = 0
            for table_name in self.tokenizer.table_list:
                if table_name in tf_dict and tf_dict[table_name].num_rows > 0:
                    table_to_node_offset[table_name] = node_offset
                    node_offset += tf_dict[table_name].num_rows
            
            # 3. RT Tokenization
            tokenizer_output = self.tokenizer(
                tf_dict,
                edge_index_dict=batch.edge_index_dict,
                table_to_node_offset=table_to_node_offset
            )
            x, node_idxs, col_idxs, table_idxs, f2p_nbr_idxs, total_rows = tokenizer_output
            
            if x is None or x.shape[0] == 0:
                print("âš ï¸ Warning: Tokenizer returned empty output")
                return None
            
            # 4. RT Layers
            masks = self.create_masks(node_idxs, col_idxs, table_idxs, f2p_nbr_idxs)
            # ç»Ÿä¸€å¤„ç†ï¼šç¡®ä¿æœ‰ batch ç»´åº¦
            if x.dim() == 2:
                x = x.unsqueeze(0)  # [1, N, Dim]
            
            # ç¡®ä¿ masks ä¹Ÿæœ‰ batch ç»´åº¦
            if isinstance(masks, dict):
                for key in masks:
                    if masks[key].dim() == 2:
                        masks[key] = masks[key].unsqueeze(0)
            
            for layer in self.rt_layers:
                x = layer(x, masks)
            
            # ç§»é™¤ batch ç»´åº¦ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼‰
            if x.shape[0] == 1:
                x = x.squeeze(0)  # [N, Dim]
            
            # 5. Aggregate (Attention-based Pooling)
            # ä½¿ç”¨ Attention Pooling æ›¿ä»£ç®€å•çš„ Mean Pooling
            # è¿™æ ·å¯ä»¥è‡ªåŠ¨å­¦ä¹ å“ªäº›èŠ‚ç‚¹æ›´é‡è¦
            if x.dim() == 2:
                x = x.unsqueeze(0)  # [1, N, Dim] for attention
            
            # ä½¿ç”¨å¯å­¦ä¹ çš„ query è¿›è¡Œ attention pooling
            query = self.pool_query.to(x.device).expand(x.shape[0], -1, -1)  # [1, 1, Dim]
            attn_out, attn_weights = self.attention_pool(
                query=query,
                key=x,
                value=x,
                need_weights=False
            )
            graph_emb = attn_out.squeeze(1)  # [1, RT_Dim]
            
            # å¦‚æœ attention å¤±è´¥ï¼Œfallback åˆ° mean pooling
            if graph_emb.shape[0] == 0:
                graph_emb = x.mean(dim=1)  # [1, RT_Dim]
            
            # 6. Project to LLM Space
            x_llm = self.projector(graph_emb)  # [1, LLM_Dim]
            
            return x_llm
            
        except Exception as e:
            print(f"âŒ Error in encode_structure: {e}")
            import traceback
            traceback.print_exc()
            return None

    def forward(self, batch, input_ids=None, labels=None, **kwargs):
        """
        è®­ç»ƒæ¨¡å¼: Causal LM Loss
        Args:
            batch: HeteroData (Graph structure) æˆ– List[HeteroData] (æ‰¹é‡å¤„ç†)
            input_ids: [B, Seq_Len] (Question + SQL tokens)
            labels: [B, Seq_Len] (SQL tokens only, Question part masked as -100)
        Returns:
            loss: torch.Tensor æˆ– None (å¦‚æœå‡ºé”™)
            logits: torch.Tensor æˆ– None
        """
        try:
            # å¤„ç† batch è¾“å…¥ï¼šæ”¯æŒå•ä¸ª HeteroData æˆ–åˆ—è¡¨
            if isinstance(batch, list):
                # å¦‚æœ batch æ˜¯åˆ—è¡¨ï¼Œç›®å‰åªå¤„ç†ç¬¬ä¸€ä¸ªï¼ˆæœªæ¥å¯ä»¥æ‰©å±•ä¸ºçœŸæ­£çš„æ‰¹é‡å¤„ç†ï¼‰
                if len(batch) == 0:
                    print("âš ï¸ Warning: Empty batch list")
                    return torch.tensor(0.0, device=self.main_device), None
                batch = batch[0]
            
            # 1. è®¡ç®—ç»“æ„ç‰¹å¾ (Soft Prompt)
            if input_ids is None:
                print("âš ï¸ Warning: input_ids is None in forward")
                return torch.tensor(0.0, device=self.main_device), None
            
            bsz = input_ids.shape[0]
            
            rt_out = self.encode_structure(batch)  # [1, LLM_Dim]
            if rt_out is None:
                # è¿”å›é›¶æŸå¤±è€Œä¸æ˜¯ Noneï¼Œé¿å…è®­ç»ƒä¸­æ–­
                print("âš ï¸ Warning: encode_structure returned None, returning zero loss")
                return torch.tensor(0.0, device=self.main_device, requires_grad=True), None
            
            # ä½¿ç”¨ repeat è€Œä¸æ˜¯ expandï¼Œç¡®ä¿å†…å­˜å®‰å…¨
            structure_prompt = rt_out.repeat(bsz, 1)  # [B, LLM_Dim]
            structure_prompt = structure_prompt.unsqueeze(1)  # [B, 1, LLM_Dim]
            
            # 2. å‡†å¤‡ Text Embeddings
            # è¿™é‡Œçš„ input_ids æ˜¯ Question + SQL
            llm_embeds = self.llm.get_input_embeddings()(input_ids)  # [B, Seq, LLM_Dim]
            
            # 3. Concat: [Structure, Text]
            inputs_embeds = torch.cat([structure_prompt, llm_embeds], dim=1)  # [B, 1+Seq, LLM_Dim]
            
            # 4. è°ƒæ•´ Labels (Shift for Soft Prompt)
            if labels is not None:
                # Soft prompt æ²¡æœ‰ label (-100)
                prefix_labels = torch.full((bsz, 1), -100, dtype=labels.dtype, device=labels.device)
                labels = torch.cat([prefix_labels, labels], dim=1)
            
            # 5. LLM Forward
            outputs = self.llm(
                inputs_embeds=inputs_embeds,
                labels=labels,
                output_hidden_states=False
            )
            
            return outputs.loss, outputs.logits
            
        except Exception as e:
            print(f"âŒ Error in forward: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é›¶æŸå¤±ï¼Œé¿å…è®­ç»ƒä¸­æ–­
            return torch.tensor(0.0, device=self.main_device, requires_grad=True), None

    @torch.no_grad()
    def generate(self, batch, question_text_list, max_new_tokens=512):
        """
        æ¨ç†æ¨¡å¼: CoT Generation
        Args:
            batch: HeteroData æˆ– List[HeteroData]
            question_text_list: List[str] è‡ªç„¶è¯­è¨€é—®é¢˜
        Returns:
            List[str]: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        try:
            self.eval()
            
            # å¤„ç† batch è¾“å…¥
            if isinstance(batch, list):
                if len(batch) == 0:
                    return ["Error: Empty batch list"] * len(question_text_list)
                batch = batch[0]  # ç›®å‰åªå¤„ç†ç¬¬ä¸€ä¸ªï¼Œæœªæ¥å¯æ‰©å±•
            
            bsz = len(question_text_list)
            if bsz == 0:
                return []
            
            # 1. ç»“æ„ç‰¹å¾
            rt_out = self.encode_structure(batch)
            if rt_out is None:
                return ["Error: No structure"] * bsz
            
            # ä½¿ç”¨ repeat ç¡®ä¿å†…å­˜å®‰å…¨
            structure_prompt = rt_out.repeat(bsz, 1).unsqueeze(1)  # [B, 1, LLM_Dim]
            
            # 2. æ–‡æœ¬ç¼–ç 
            # æ„é€  Prompt: "Question: ... \n Answer:"
            # DeepSeek-R1 å»ºè®®çš„ prompt æ ¼å¼
            prompts = [f"Question: {q}\nAnswer:" for q in question_text_list]
            inputs = self.llm_tokenizer(prompts, return_tensors="pt", padding=True).to(self.main_device)
            
            text_embeds = self.llm.get_input_embeddings()(inputs.input_ids)
            
            # 3. Concat
            inputs_embeds = torch.cat([structure_prompt, text_embeds], dim=1)
            
            # Attention Mask (ç»™ soft prompt è¡¥ 1)
            soft_prompt_mask = torch.ones((bsz, 1), device=self.main_device, dtype=inputs.attention_mask.dtype)
            attention_mask = torch.cat([soft_prompt_mask, inputs.attention_mask], dim=1)
            
            # 4. Generate
            # DeepSeek ä¼šè‡ªåŠ¨è¾“å‡º <think>...</think>
            outputs = self.llm.generate(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.llm_tokenizer.eos_token_id,
                do_sample=False  # Greedy decoding for stability in SQL
            )
            
            # 5. Decode
            decoded = self.llm_tokenizer.batch_decode(outputs, skip_special_tokens=True)
            return decoded
            
        except Exception as e:
            print(f"âŒ Error in generate: {e}")
            import traceback
            traceback.print_exc()
            return [f"Error: {str(e)}"] * len(question_text_list) if question_text_list else []
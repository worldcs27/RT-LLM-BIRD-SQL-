import os
import json
import sqlite3
import torch
import numpy as np
import networkx as nx
import hashlib
from functools import lru_cache
from typing import Dict, List, Tuple, Any, Optional
from torch_geometric.data import HeteroData
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from threading import Lock

# å°è¯•å¯¼å…¥ torch_frameï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ Mock
try:
    from torch_frame import stype
except ImportError:
    class stype:
        numerical = "numerical"
        categorical = "categorical"
        text_embedded = "text_embedded"

# --- Mock TensorFrame ä»¥å…¼å®¹ model_clean.py ---
class MockTensorFrame:
    def __init__(self, feat_dict: Dict[str, torch.Tensor], col_names_dict: Dict[str, List[str]]):
        self.feat_dict = feat_dict
        self.col_names_dict = col_names_dict
        # è®¡ç®—è¡Œæ•° (å‡è®¾æ‰€æœ‰ç‰¹å¾è¡Œæ•°ä¸€è‡´)
        self.num_rows = 0
        if feat_dict:
            first_key = next(iter(feat_dict))
            self.num_rows = feat_dict[first_key].shape[0]
        self.device = torch.device('cpu')

    def to(self, device):
        self.device = device
        for k, v in self.feat_dict.items():
            self.feat_dict[k] = v.to(device)
        return self

    def __getitem__(self, index):
        # æ”¯æŒåˆ‡ç‰‡ä»¥å…¼å®¹ RTEmbedding çš„å†…éƒ¨é€»è¾‘
        new_feat_dict = {}
        for k, v in self.feat_dict.items():
            new_feat_dict[k] = v[index]
        return MockTensorFrame(new_feat_dict, self.col_names_dict)

# --- æ•°æ®åº“è¿æ¥æ±  ---
class DatabaseConnectionPool:
    """ç®€å•çš„æ•°æ®åº“è¿æ¥æ± ï¼Œå¤ç”¨è¿æ¥ä»¥å‡å°‘å¼€é”€"""
    def __init__(self, max_connections=10):
        self.max_connections = max_connections
        self.pools = {}  # {db_path: [conn1, conn2, ...]}
        self.lock = Lock()
    
    def get_connection(self, db_path: str) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä»æ± ä¸­è·å–æˆ–åˆ›å»ºæ–°è¿æ¥ï¼‰"""
        with self.lock:
            if db_path not in self.pools:
                self.pools[db_path] = []
            
            pool = self.pools[db_path]
            if pool:
                conn = pool.pop()
                # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
                try:
                    conn.execute("SELECT 1")
                    return conn
                except:
                    # è¿æ¥å·²å…³é—­ï¼Œåˆ›å»ºæ–°çš„
                    pass
            
            # åˆ›å»ºæ–°è¿æ¥
            conn = sqlite3.connect(db_path, check_same_thread=False)
            return conn
    
    def return_connection(self, db_path: str, conn: sqlite3.Connection):
        """å½’è¿˜è¿æ¥åˆ°æ± ä¸­"""
        with self.lock:
            if db_path not in self.pools:
                self.pools[db_path] = []
            
            pool = self.pools[db_path]
            if len(pool) < self.max_connections:
                pool.append(conn)
            else:
                conn.close()
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        with self.lock:
            for pool in self.pools.values():
                for conn in pool:
                    conn.close()
            self.pools.clear()

# --- æ ¸å¿ƒé€‚é…å™¨ç±» ---
class BirdSQLAdapter:
    def __init__(self, bird_root_path: str, deepseek_model_path: str = "deepseek-ai/DeepSeek-Coder-V2-Lite-Base"):
        """
        Args:
            bird_root_path: BIRDæ•°æ®é›†æ ¹ç›®å½• (åŒ…å« train/train_tables.json ç­‰)
            deepseek_model_path: ç”¨äºè¯­ä¹‰å¯¹é½çš„ DeepSeek æ¨¡å‹è·¯å¾„
        """
        self.root = bird_root_path
        self.train_tables_path = os.path.join(self.root, "train", "train_tables.json")
        self.train_db_root = os.path.join(self.root, "train", "train_databases")
        self.train_json_path = os.path.join(self.root, "train", "train.json")
        
        # å®šä¹‰ç¼“å­˜ç›®å½•
        self.cache_dir = os.path.join(self.root, "cache_deepseek_rt")
        os.makedirs(self.cache_dir, exist_ok=True)
        
        print(f"ğŸš€ Initializing BIRD-SQL Adapter...")
        print(f"   - Root: {self.root}")
        print(f"   - Cache Dir: {self.cache_dir}")
        print(f"   - Embedding Model: {deepseek_model_path}")

        # 1. åŠ è½½ DeepSeek è¯­ä¹‰ç¼–ç å™¨
        # [å†…å­˜ä¼˜åŒ–] ä½¿ç”¨ CPU offloadingï¼Œå‡å°‘ GPU å†…å­˜å ç”¨
        # æ¨¡å‹ä¼šåŠ è½½åˆ° CPUï¼Œåªåœ¨éœ€è¦æ—¶ä¸´æ—¶ç§»åŠ¨åˆ° GPU
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                deepseek_model_path, 
                trust_remote_code=True,
                local_files_only=True
            )
            # [å…³é”®] ä½¿ç”¨ CPU offloadingï¼Œé¿å…å ç”¨ GPU å†…å­˜
            self.model = AutoModel.from_pretrained(
                deepseek_model_path, 
                trust_remote_code=True, 
                device_map="cpu",  # å…ˆåŠ è½½åˆ° CPU
                local_files_only=True,
                torch_dtype=torch.float16  # ä½¿ç”¨ float16 å‡å°‘å†…å­˜
            )
            print("   âœ… Adapter model loaded to CPU (will move to GPU only when needed)")
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to load local model: {e}")
            print("   Trying to load with network access...")
            self.tokenizer = AutoTokenizer.from_pretrained(deepseek_model_path, trust_remote_code=True)
            self.model = AutoModel.from_pretrained(
                deepseek_model_path, 
                trust_remote_code=True, 
                device_map="cpu",  # CPU offloading
                torch_dtype=torch.float16
            )
            
        self.model.eval()
        
        # 2. è§£æ Schema Graph
        self.schemas, self.graphs = self._load_and_build_graphs()
        
        # 3. é¢„è®¡ç®— Schema Embeddings (å¸¦ç£ç›˜ç¼“å­˜)
        self.schema_embeddings = self._precompute_schema_embeddings()
        
        # 4. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        self.db_pool = DatabaseConnectionPool(max_connections=10)
        
        # 5. é—®é¢˜ç¼–ç ç¼“å­˜
        self._question_cache = {}
        self._cache_lock = Lock()

    def _encode_text(self, texts: List[str], device="cuda", use_cache=True) -> torch.Tensor:
        """ä½¿ç”¨ DeepSeek è·å–æ–‡æœ¬åµŒå…¥"""
        if len(texts) == 1 and use_cache:
            text = texts[0]
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
            with self._cache_lock:
                if text_hash in self._question_cache:
                    return self._question_cache[text_hash]
        
        # [ä¿®å¤] å®‰å…¨è·å–æ¨¡å‹è®¾å¤‡
        if self.model is None:
            raise RuntimeError("Model is not loaded. Cannot encode text.")
        
        # [å†…å­˜ä¼˜åŒ–] å¦‚æœæ¨¡å‹åœ¨ CPU ä¸Šï¼Œä¸´æ—¶ç§»åŠ¨åˆ° GPU è¿›è¡Œç¼–ç 
        model_was_on_cpu = False
        target_device = torch.device(device) if isinstance(device, str) else device
        
        try:
            first_param = next(self.model.parameters(), None)
            if first_param is not None:
                current_device = first_param.device
                # å¦‚æœæ¨¡å‹åœ¨ CPU ä¸Šï¼Œä¸”ç›®æ ‡è®¾å¤‡æ˜¯ GPUï¼Œä¸´æ—¶ç§»åŠ¨
                if current_device.type == 'cpu' and target_device.type == 'cuda':
                    model_was_on_cpu = True
                    # ä¸´æ—¶ç§»åŠ¨åˆ° GPUï¼ˆåªç§»åŠ¨å¿…è¦çš„å±‚ï¼‰
                    self.model = self.model.to(target_device)
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
        except Exception as e:
            print(f"   âš ï¸  Warning: Could not check/move model device: {e}")
        
        batch_size = 32
        all_embs = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                inputs = self.tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128)
                # å°†è¾“å…¥ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
                inputs = {k: v.to(target_device) for k, v in inputs.items()}
                
                outputs = self.model(**inputs, output_hidden_states=True, use_cache=False)
                last_hidden = outputs.last_hidden_state
                mask = inputs.attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
                sum_embeddings = torch.sum(last_hidden * mask, 1)
                sum_mask = torch.clamp(mask.sum(1), min=1e-9)
                embs = sum_embeddings / sum_mask
                all_embs.append(embs.cpu())
        
        result = torch.cat(all_embs, dim=0)
        
        # [å†…å­˜ä¼˜åŒ–] å¦‚æœæ¨¡å‹æ˜¯ä» CPU ä¸´æ—¶ç§»åŠ¨åˆ° GPU çš„ï¼Œç¼–ç å®Œæˆåç§»å› CPU
        if model_was_on_cpu:
            try:
                self.model = self.model.cpu()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()  # æ¸…ç† GPU ç¼“å­˜
            except Exception as e:
                print(f"   âš ï¸  Warning: Could not move model back to CPU: {e}")
        
        if len(texts) == 1 and use_cache:
            text_hash = hashlib.md5(texts[0].encode('utf-8')).hexdigest()
            with self._cache_lock:
                if len(self._question_cache) >= 1000:
                    oldest_key = next(iter(self._question_cache))
                    del self._question_cache[oldest_key]
                self._question_cache[text_hash] = result
        
        return result

    def _load_and_build_graphs(self):
        """è§£æ JSON æ„å»º NetworkX å›¾"""
        print("ğŸ“Š Parsing Schema and Building Graphs...")
        with open(self.train_tables_path, 'r', encoding='utf-8') as f:
            tables_data = json.load(f)
            
        schemas = {}
        graphs = {}
        
        for db in tables_data:
            db_id = db['db_id']
            schemas[db_id] = db
            G = nx.Graph() 
            for i, tbl_name in enumerate(db['table_names']):
                G.add_node(i, name=tbl_name, type='table')
            
            column_names = db['column_names']
            for src_col_idx, dst_col_idx in db['foreign_keys']:
                src_tbl_idx = column_names[src_col_idx][0]
                dst_tbl_idx = column_names[dst_col_idx][0]
                if src_tbl_idx != dst_tbl_idx:
                    G.add_edge(src_tbl_idx, dst_tbl_idx, type='fk')
            graphs[db_id] = G
            
        return schemas, graphs

    def _precompute_schema_embeddings(self):
        """é¢„è®¡ç®—æ‰€æœ‰è¡¨åå’Œåˆ—åçš„ Embedding (å¸¦ç£ç›˜ç¼“å­˜)"""
        cache_file = os.path.join(self.cache_dir, "schema_embeddings.pt")
        
        if os.path.exists(cache_file):
            print(f"ğŸ’¾ Loading cached schema embeddings from {cache_file}...")
            return torch.load(cache_file)
            
        print("ğŸ§  Precomputing Schema Embeddings (First Time Run)...")
        cache = {}
        
        for db_id, schema in tqdm(self.schemas.items()):
            table_texts = [f"Table: {name}" for name in schema['table_names']]
            col_texts = []
            for idx, (tbl_idx, col_name) in enumerate(schema['column_names']):
                col_type = schema['column_types'][idx]
                col_texts.append(f"Column: {col_name} Type: {col_type}")
                
            t_embs = self._encode_text(table_texts)
            c_embs = self._encode_text(col_texts)
            
            cache[db_id] = {
                'table_embs': t_embs,
                'col_embs': c_embs
            }
        
        print(f"ğŸ’¾ Saving schema embeddings to {cache_file}...")
        torch.save(cache, cache_file)
        return cache

    def get_all_schema_metadata(self):
        """
        [æ–°å¢] è·å–æ‰€æœ‰æ•°æ®åº“çš„ Schema å…ƒæ•°æ®
        ç”¨äºæ¨¡å‹åˆå§‹åŒ–æ—¶æ„å»ºå‚æ•° (RTEmbedding)ï¼Œè§£å†³ 'NoneType' æŠ¥é”™
        """
        meta_data = HeteroData()
        print("ğŸ“¦ Constructing Global Schema Metadata for Initialization...")
        
        # ä½¿ç”¨ set é¿å…é‡å¤å¤„ç†åŒåè¡¨
        processed_tables = set()
        
        for db_id, schema in self.schemas.items():
            table_names = schema['table_names']
            column_names = schema['column_names'] # [table_idx, col_name]
            
            for t_idx, t_name in enumerate(table_names):
                if t_name in processed_tables:
                    continue
                processed_tables.add(t_name)
                
                # æ”¶é›†è¯¥è¡¨çš„æ‰€æœ‰åˆ—å
                curr_cols = []
                for c_idx, (tbl_idx, c_name) in enumerate(column_names):
                    if tbl_idx == t_idx:
                        curr_cols.append(c_name)
                
                # æ„é€  MockTensorFrame å…ƒæ•°æ®
                col_names_dict = {
                    stype.text_embedded: curr_cols
                }
                feat_dict = {} # ç©ºå­—å…¸ï¼Œå› ä¸ºåˆå§‹åŒ–ä¸éœ€è¦çœŸå®æ•°æ®
                
                meta_data[t_name].tf = MockTensorFrame(feat_dict, col_names_dict)
                meta_data[t_name].num_nodes = 0
                
        return meta_data

    def prune_schema(self, question: str, db_id: str, top_k_tables=4) -> Tuple[List[int], List[int]]:
        """Question-Aware Schema Pruning (2-hop)"""
        if db_id not in self.schemas:
            return [], []
            
        schema = self.schemas[db_id]
        G = self.graphs[db_id]
        cached_embs = self.schema_embeddings[db_id]
        
        q_emb = self._encode_text([question], use_cache=True)[0]
        t_sim = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), cached_embs['table_embs'])
        
        num_tables = len(schema['table_names'])
        k = min(top_k_tables, num_tables)
        anchor_table_indices = torch.topk(t_sim, k).indices.tolist()
        
        active_tables = set(anchor_table_indices)
        neighbors = set()
        for t_idx in anchor_table_indices:
            if t_idx in G:
                for nbr in G.neighbors(t_idx):
                    neighbors.add(nbr)
        active_tables.update(neighbors)
        
        if len(active_tables) < 5:
            second_hop = set()
            for t_idx in neighbors:
                if t_idx in G:
                    for nbr in G.neighbors(t_idx):
                        second_hop.add(nbr)
            active_tables.update(second_hop)
            
        active_table_indices = sorted(list(active_tables))
        active_col_indices = []
        for col_idx, (tbl_idx, _) in enumerate(schema['column_names']):
            if tbl_idx in active_table_indices:
                active_col_indices.append(col_idx)
                
        return active_table_indices, active_col_indices
    
    def prune_schema_batch(self, questions: List[str], db_ids: List[str], top_k_tables=4) -> List[Tuple[List[int], List[int]]]:
        """æ‰¹é‡ Schema Pruning"""
        if len(questions) != len(db_ids):
            raise ValueError("questions and db_ids must have the same length")
        
        results = []
        unique_questions = list(set(questions))
        question_to_emb = {}
        if unique_questions:
            batch_embs = self._encode_text(unique_questions, use_cache=False)
            for q, emb in zip(unique_questions, batch_embs):
                question_to_emb[q] = emb
        
        for question, db_id in zip(questions, db_ids):
            if db_id not in self.schemas:
                results.append(([], []))
                continue
            schema = self.schemas[db_id]
            G = self.graphs[db_id]
            cached_embs = self.schema_embeddings[db_id]
            q_emb = question_to_emb[question]
            
            t_sim = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), cached_embs['table_embs'])
            num_tables = len(schema['table_names'])
            k = min(top_k_tables, num_tables)
            anchor_table_indices = torch.topk(t_sim, k).indices.tolist()
            
            active_tables = set(anchor_table_indices)
            neighbors = set()
            for t_idx in anchor_table_indices:
                if t_idx in G:
                    for nbr in G.neighbors(t_idx):
                        neighbors.add(nbr)
            active_tables.update(neighbors)
            
            if len(active_tables) < 5:
                second_hop = set()
                for t_idx in neighbors:
                    if t_idx in G:
                        for nbr in G.neighbors(t_idx):
                            second_hop.add(nbr)
                active_tables.update(second_hop)
            
            active_table_indices = sorted(list(active_tables))
            active_col_indices = [col_idx for col_idx, (tbl_idx, _) in enumerate(schema['column_names']) if tbl_idx in active_table_indices]
            results.append((active_table_indices, active_col_indices))
        return results

    def get_sample_hetero_data(self, question: str, db_id: str):
        """æ„å»ºå•ä¸ª HeteroData å¯¹è±¡"""
        active_table_idxs, active_col_idxs = self.prune_schema(question, db_id)
        return self._build_hetero_data_single(db_id, active_table_idxs, active_col_idxs)
    
    def get_sample_hetero_data_batch(self, questions: List[str], db_ids: List[str]) -> List[HeteroData]:
        """æ‰¹é‡æ„å»º HeteroData å¯¹è±¡"""
        pruning_results = self.prune_schema_batch(questions, db_ids)
        results = []
        for question, db_id, (active_table_idxs, active_col_idxs) in zip(questions, db_ids, pruning_results):
            try:
                data = self._build_hetero_data_single(db_id, active_table_idxs, active_col_idxs)
                results.append(data)
            except Exception as e:
                print(f"âš ï¸ Warning: Failed to build HeteroData for question '{question[:50]}...' in {db_id}: {e}")
                results.append(HeteroData()) 
        return results
    
    def _build_hetero_data_single(self, db_id: str, active_table_idxs: List[int], active_col_idxs: List[int]) -> HeteroData:
        """ä¸ºå•ä¸ªé—®é¢˜æ„å»º HeteroData (å†…éƒ¨æ–¹æ³•)"""
        schema = self.schemas[db_id]
        data = HeteroData()
        col_names = schema['column_names']
        
        for t_idx in active_table_idxs:
            table_name = schema['table_names'][t_idx]
            curr_table_col_idxs = [i for i in active_col_idxs if col_names[i][0] == t_idx]
            
            if not curr_table_col_idxs:
                continue
            
            db_path = os.path.join(self.train_db_root, db_id, f"{db_id}.sqlite")
            conn = self.db_pool.get_connection(db_path)
            cursor = conn.cursor()
            
            try:
                cursor.execute(f"SELECT * FROM `{table_name}` LIMIT 3")
                rows = cursor.fetchall()
            except Exception as e:
                # print(f"âš ï¸ Warning: Failed to read from table {table_name}: {e}")
                rows = []
            finally:
                self.db_pool.return_connection(db_path, conn)
            
            num_rows = max(len(rows), 1)
            
            table_col_embs = []
            for c_idx in curr_table_col_idxs:
                emb = self.schema_embeddings[db_id]['col_embs'][c_idx]
                table_col_embs.append(emb)
            
            if not table_col_embs:
                continue
            
            col_feats = torch.stack(table_col_embs)
            col_feats = col_feats.unsqueeze(0).expand(num_rows, -1, -1)
            
            feat_dict = {stype.text_embedded: col_feats.float()}
            c_names = [col_names[i][1] for i in curr_table_col_idxs]
            col_names_dict = {stype.text_embedded: c_names}
            
            data[table_name].tf = MockTensorFrame(feat_dict, col_names_dict)
            data[table_name].num_nodes = num_rows
        
        for src_col, dst_col in schema['foreign_keys']:
            src_t_idx = col_names[src_col][0]
            dst_t_idx = col_names[dst_col][0]
            
            if src_t_idx in active_table_idxs and dst_t_idx in active_table_idxs:
                src_name = schema['table_names'][src_t_idx]
                dst_name = schema['table_names'][dst_t_idx]
                
                if src_name in data.node_types and dst_name in data.node_types:
                    edge_index = torch.tensor([[0], [0]], dtype=torch.long)
                    data[src_name, "fkey", dst_name].edge_index = edge_index
                    data[dst_name, "rev_fkey", src_name].edge_index = edge_index
        
        return data
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'db_pool'):
            self.db_pool.close_all()

# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    BIRD_ROOT = "/data/cuishuai/datasets/text-to-sql/BIRD-SQL" 
    TEST_MODEL = "sentence-transformers/all-MiniLM-L6-v2" 
    if os.path.exists(BIRD_ROOT):
        adapter = BirdSQLAdapter(BIRD_ROOT, TEST_MODEL)
        q = "How many customers?"
        db_id = list(adapter.schemas.keys())[0]
        t_idxs, c_idxs = adapter.prune_schema(q, db_id)
        print(f"Selected Tables: {t_idxs}")
import os
import json
import sqlite3
import torch
import numpy as np
import networkx as nx
import hashlib
from functools import lru_cache
from typing import Dict, List, Tuple, Any, Optional
from torch_geometric.data import HeteroData
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from threading import Lock

# å°è¯•å¯¼å…¥ torch_frameï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ Mock
try:
    from torch_frame import stype
except ImportError:
    class stype:
        numerical = "numerical"
        categorical = "categorical"
        text_embedded = "text_embedded"

# --- Mock TensorFrame ä»¥å…¼å®¹ model_clean.py ---
class MockTensorFrame:
    def __init__(self, feat_dict: Dict[str, torch.Tensor], col_names_dict: Dict[str, List[str]]):
        self.feat_dict = feat_dict
        self.col_names_dict = col_names_dict
        # è®¡ç®—è¡Œæ•° (å‡è®¾æ‰€æœ‰ç‰¹å¾è¡Œæ•°ä¸€è‡´)
        self.num_rows = 0
        if feat_dict:
            first_key = next(iter(feat_dict))
            self.num_rows = feat_dict[first_key].shape[0]
        self.device = torch.device('cpu')

    def to(self, device):
        self.device = device
        for k, v in self.feat_dict.items():
            self.feat_dict[k] = v.to(device)
        return self

    def __getitem__(self, index):
        # æ”¯æŒåˆ‡ç‰‡ä»¥å…¼å®¹ RTEmbedding çš„å†…éƒ¨é€»è¾‘
        new_feat_dict = {}
        for k, v in self.feat_dict.items():
            new_feat_dict[k] = v[index]
        return MockTensorFrame(new_feat_dict, self.col_names_dict)

# --- æ•°æ®åº“è¿æ¥æ±  ---
class DatabaseConnectionPool:
    """ç®€å•çš„æ•°æ®åº“è¿æ¥æ± ï¼Œå¤ç”¨è¿æ¥ä»¥å‡å°‘å¼€é”€"""
    def __init__(self, max_connections=10):
        self.max_connections = max_connections
        self.pools = {}  # {db_path: [conn1, conn2, ...]}
        self.lock = Lock()
    
    def get_connection(self, db_path: str) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä»æ± ä¸­è·å–æˆ–åˆ›å»ºæ–°è¿æ¥ï¼‰"""
        with self.lock:
            if db_path not in self.pools:
                self.pools[db_path] = []
            
            pool = self.pools[db_path]
            if pool:
                conn = pool.pop()
                # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
                try:
                    conn.execute("SELECT 1")
                    return conn
                except:
                    # è¿æ¥å·²å…³é—­ï¼Œåˆ›å»ºæ–°çš„
                    pass
            
            # åˆ›å»ºæ–°è¿æ¥
            conn = sqlite3.connect(db_path, check_same_thread=False)
            return conn
    
    def return_connection(self, db_path: str, conn: sqlite3.Connection):
        """å½’è¿˜è¿æ¥åˆ°æ± ä¸­"""
        with self.lock:
            if db_path not in self.pools:
                self.pools[db_path] = []
            
            pool = self.pools[db_path]
            if len(pool) < self.max_connections:
                pool.append(conn)
            else:
                conn.close()
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        with self.lock:
            for pool in self.pools.values():
                for conn in pool:
                    conn.close()
            self.pools.clear()

# --- æ ¸å¿ƒé€‚é…å™¨ç±» ---
class BirdSQLAdapter:
    def __init__(self, bird_root_path: str, deepseek_model_path: str = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"):
        """
        Args:
            bird_root_path: BIRDæ•°æ®é›†æ ¹ç›®å½• (åŒ…å« train/train_tables.json ç­‰)
            deepseek_model_path: ç”¨äºè¯­ä¹‰å¯¹é½çš„ DeepSeek æ¨¡å‹è·¯å¾„ (å¯ç”¨è¾ƒå°çš„ç‰ˆæœ¬ä»¥èŠ‚çœæ˜¾å­˜)
        """
        self.root = bird_root_path
        self.train_tables_path = os.path.join(self.root, "train", "train_tables.json")
        self.train_db_root = os.path.join(self.root, "train", "train_databases")
        self.train_json_path = os.path.join(self.root, "train", "train.json")
        
        print(f"ğŸš€ Initializing BIRD-SQL Adapter...")
        print(f"   - Root: {self.root}")
        print(f"   - Embedding Model: {deepseek_model_path}")

        # 1. åŠ è½½ DeepSeek è¯­ä¹‰ç¼–ç å™¨ (ç”¨äº Schema Pruning å’Œ Feature Initialization)
        self.tokenizer = AutoTokenizer.from_pretrained(deepseek_model_path, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(deepseek_model_path, trust_remote_code=True, device_map="auto")
        self.model.eval()
        
        # 2. è§£æ Schema Graph
        self.schemas, self.graphs = self._load_and_build_graphs()
        
        # 3. é¢„è®¡ç®— Schema Embeddings (Table & Column Names)
        #    è¿™å¯¹äº Pruning æ—¶çš„ç›¸ä¼¼åº¦è®¡ç®—è‡³å…³é‡è¦
        self.schema_embeddings = self._precompute_schema_embeddings()
        
        # 4. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        self.db_pool = DatabaseConnectionPool(max_connections=10)
        
        # 5. é—®é¢˜ç¼–ç ç¼“å­˜ (ä½¿ç”¨å“ˆå¸Œé¿å…é‡å¤ç¼–ç )
        self._question_cache = {}
        self._cache_lock = Lock()

    def _encode_text(self, texts: List[str], device="cuda", use_cache=True) -> torch.Tensor:
        """ä½¿ç”¨ DeepSeek è·å–æ–‡æœ¬åµŒå…¥ (Last Hidden State Mean Pooling)
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            device: è®¾å¤‡
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆä»…å¯¹å•ä¸ªæ–‡æœ¬æœ‰æ•ˆï¼‰
        """
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬ä¸”å¯ç”¨ç¼“å­˜ï¼Œå°è¯•ä»ç¼“å­˜è·å–
        if len(texts) == 1 and use_cache:
            text = texts[0]
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
            
            with self._cache_lock:
                if text_hash in self._question_cache:
                    return self._question_cache[text_hash]
        
        # æ‰¹é‡ç¼–ç 
        batch_size = 32
        all_embs = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                inputs = self.tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128).to(self.model.device)
                outputs = self.model(**inputs, output_hidden_states=True)
                # ä½¿ç”¨æœ€åä¸€å±‚çš„ Mean Pooling
                last_hidden = outputs.last_hidden_state # [B, Seq, Dim]
                mask = inputs.attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
                sum_embeddings = torch.sum(last_hidden * mask, 1)
                sum_mask = torch.clamp(mask.sum(1), min=1e-9)
                embs = sum_embeddings / sum_mask
                all_embs.append(embs.cpu())
        
        result = torch.cat(all_embs, dim=0)
        
        # ç¼“å­˜å•ä¸ªæ–‡æœ¬çš„ç»“æœ
        if len(texts) == 1 and use_cache:
            text_hash = hashlib.md5(texts[0].encode('utf-8')).hexdigest()
            with self._cache_lock:
                # é™åˆ¶ç¼“å­˜å¤§å°ï¼ˆLRU ç­–ç•¥ï¼šä¿ç•™æœ€è¿‘ 1000 ä¸ªï¼‰
                if len(self._question_cache) >= 1000:
                    # åˆ é™¤æœ€æ—§çš„ï¼ˆç®€å•ç­–ç•¥ï¼šåˆ é™¤ç¬¬ä¸€ä¸ªï¼‰
                    oldest_key = next(iter(self._question_cache))
                    del self._question_cache[oldest_key]
                self._question_cache[text_hash] = result
        
        return result

    def _load_and_build_graphs(self):
        """è§£æ JSON æ„å»º NetworkX å›¾ (ç”¨äº 2-hop æ‰©å±•)"""
        print("ğŸ“Š Parsing Schema and Building Graphs...")
        with open(self.train_tables_path, 'r', encoding='utf-8') as f:
            tables_data = json.load(f)
            
        schemas = {}
        graphs = {}
        
        for db in tables_data:
            db_id = db['db_id']
            schemas[db_id] = db
            
            # æ„å»ºæ— å‘å›¾ç”¨äº BFS
            G = nx.Graph() 
            
            # æ·»åŠ èŠ‚ç‚¹ (Table)
            for i, tbl_name in enumerate(db['table_names']):
                G.add_node(i, name=tbl_name, type='table')
                
            # æ·»åŠ è¾¹ (Foreign Keys)
            # foreign_keys: [[col_idx_src, col_idx_dst], ...]
            column_names = db['column_names'] # [table_idx, col_name]
            for src_col_idx, dst_col_idx in db['foreign_keys']:
                src_tbl_idx = column_names[src_col_idx][0]
                dst_tbl_idx = column_names[dst_col_idx][0]
                
                # å¿½ç•¥è‡ªç¯æˆ–åŒä¸€ä¸ªè¡¨çš„å…³è” (è™½ç„¶ BIRD é‡Œä¸å¤š)
                if src_tbl_idx != dst_tbl_idx:
                    G.add_edge(src_tbl_idx, dst_tbl_idx, type='fk')
            
            graphs[db_id] = G
            
        return schemas, graphs

    def _precompute_schema_embeddings(self):
        """é¢„è®¡ç®—æ‰€æœ‰è¡¨åå’Œåˆ—åçš„ Embedding"""
        print("ğŸ§  Precomputing Schema Embeddings...")
        cache = {}
        
        for db_id, schema in tqdm(self.schemas.items()):
            # 1. ç¼–ç è¡¨å
            table_texts = [f"Table: {name}" for name in schema['table_names']]
            # 2. ç¼–ç åˆ—å (Column: name Type: type)
            col_texts = []
            for idx, (tbl_idx, col_name) in enumerate(schema['column_names']):
                col_type = schema['column_types'][idx]
                col_texts.append(f"Column: {col_name} Type: {col_type}")
                
            # æ‰¹é‡ç¼–ç 
            t_embs = self._encode_text(table_texts)
            c_embs = self._encode_text(col_texts)
            
            cache[db_id] = {
                'table_embs': t_embs,
                'col_embs': c_embs
            }
        return cache

    def prune_schema(self, question: str, db_id: str, top_k_tables=4) -> Tuple[List[int], List[int]]:
        """
        æ ¸å¿ƒå‡½æ•°ï¼šQuestion-Aware Schema Pruning (2-hop)
        
        Returns:
            active_table_indices: é€‰ä¸­çš„è¡¨ç´¢å¼•åˆ—è¡¨
            active_col_indices: é€‰ä¸­çš„åˆ—ç´¢å¼•åˆ—è¡¨
        """
        if db_id not in self.schemas:
            return [], []
            
        schema = self.schemas[db_id]
        G = self.graphs[db_id]
        cached_embs = self.schema_embeddings[db_id]
        
        # 1. ç¼–ç é—®é¢˜ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        q_emb = self._encode_text([question], use_cache=True)[0] # [Dim]
        
        # 2. é”šç‚¹è¯†åˆ« (Anchor Identification)
        # è®¡ç®—é—®é¢˜ä¸è¡¨åçš„ç›¸ä¼¼åº¦
        t_sim = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), cached_embs['table_embs'])
        
        # é€‰å‡º Top-K è¡¨ä½œä¸º Anchors
        # ä¹Ÿå¯ä»¥åŠ å…¥ Column ç›¸ä¼¼åº¦æ¥è¾…åŠ©ï¼Œè¿™é‡Œå…ˆåªç”¨ Table ç®€åŒ–
        num_tables = len(schema['table_names'])
        k = min(top_k_tables, num_tables)
        anchor_table_indices = torch.topk(t_sim, k).indices.tolist()
        
        # 3. ç»“æ„æ‰©å±• (2-hop Expansion)
        active_tables = set(anchor_table_indices)
        
        # 1-hop
        neighbors = set()
        for t_idx in anchor_table_indices:
            if t_idx in G:
                for nbr in G.neighbors(t_idx):
                    neighbors.add(nbr)
        active_tables.update(neighbors)
        
        # 2-hop (Optional: å¦‚æœè¡¨å¤ªå°‘ï¼Œå†æ‰©ä¸€åœˆ)
        if len(active_tables) < 5:
            second_hop = set()
            for t_idx in neighbors:
                if t_idx in G:
                    for nbr in G.neighbors(t_idx):
                        second_hop.add(nbr)
            active_tables.update(second_hop)
            
        active_table_indices = sorted(list(active_tables))
        
        # 4. ç¡®å®š Active Columns
        # é»˜è®¤è§„åˆ™ï¼šå¦‚æœè¡¨è¢«é€‰ä¸­ï¼Œåˆ™ä¿ç•™è¯¥è¡¨çš„æ‰€æœ‰åˆ—
        # (è¿›é˜¶è§„åˆ™ï¼šåªä¿ç•™ High Similarity Columns + Primary Keys + Foreign Keys)
        active_col_indices = []
        for col_idx, (tbl_idx, _) in enumerate(schema['column_names']):
            if tbl_idx in active_table_indices:
                active_col_indices.append(col_idx)
                
        return active_table_indices, active_col_indices
    
    def prune_schema_batch(self, questions: List[str], db_ids: List[str], top_k_tables=4) -> List[Tuple[List[int], List[int]]]:
        """
        æ‰¹é‡ Schema Pruningï¼ˆä¼˜åŒ–ï¼šå…±äº«é—®é¢˜ç¼–ç ï¼‰
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            db_ids: æ•°æ®åº“IDåˆ—è¡¨ï¼ˆä¸questionsé•¿åº¦ç›¸åŒï¼‰
            top_k_tables: Top-K è¡¨æ•°é‡
            
        Returns:
            List[Tuple[List[int], List[int]]]: æ¯ä¸ªé—®é¢˜çš„ (active_table_indices, active_col_indices)
        """
        if len(questions) != len(db_ids):
            raise ValueError("questions and db_ids must have the same length")
        
        results = []
        
        # æ‰¹é‡ç¼–ç æ‰€æœ‰é—®é¢˜ï¼ˆåˆ©ç”¨æ‰¹å¤„ç†æ•ˆç‡ï¼‰
        unique_questions = list(set(questions))
        question_to_emb = {}
        if unique_questions:
            batch_embs = self._encode_text(unique_questions, use_cache=False)
            for q, emb in zip(unique_questions, batch_embs):
                question_to_emb[q] = emb
        
        # ä¸ºæ¯ä¸ªé—®é¢˜æ‰§è¡Œ Pruning
        for question, db_id in zip(questions, db_ids):
            if db_id not in self.schemas:
                results.append(([], []))
                continue
                
            schema = self.schemas[db_id]
            G = self.graphs[db_id]
            cached_embs = self.schema_embeddings[db_id]
            
            # ä½¿ç”¨é¢„ç¼–ç çš„é—®é¢˜åµŒå…¥
            q_emb = question_to_emb[question]
            
            # é”šç‚¹è¯†åˆ«
            t_sim = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), cached_embs['table_embs'])
            num_tables = len(schema['table_names'])
            k = min(top_k_tables, num_tables)
            anchor_table_indices = torch.topk(t_sim, k).indices.tolist()
            
            # ç»“æ„æ‰©å±•
            active_tables = set(anchor_table_indices)
            neighbors = set()
            for t_idx in anchor_table_indices:
                if t_idx in G:
                    for nbr in G.neighbors(t_idx):
                        neighbors.add(nbr)
            active_tables.update(neighbors)
            
            if len(active_tables) < 5:
                second_hop = set()
                for t_idx in neighbors:
                    if t_idx in G:
                        for nbr in G.neighbors(t_idx):
                            second_hop.add(nbr)
                active_tables.update(second_hop)
            
            active_table_indices = sorted(list(active_tables))
            active_col_indices = [
                col_idx for col_idx, (tbl_idx, _) in enumerate(schema['column_names'])
                if tbl_idx in active_table_indices
            ]
            
            results.append((active_table_indices, active_col_indices))
        
        return results

    def get_sample_hetero_data(self, question: str, db_id: str):
        """
        æ„å»º model_clean.py æ‰€éœ€çš„ HeteroData å¯¹è±¡
        åŒ…å« DeepSeek åˆå§‹åŒ–çš„ç‰¹å¾
        
        Args:
            question: è‡ªç„¶è¯­è¨€é—®é¢˜
            db_id: æ•°æ®åº“ID
            
        Returns:
            HeteroData: å›¾æ•°æ®å¯¹è±¡
        """
        # 1. Pruning
        active_table_idxs, active_col_idxs = self.prune_schema(question, db_id)
        
        # 2. ä½¿ç”¨å†…éƒ¨æ–¹æ³•æ„å»º HeteroData
        return self._build_hetero_data_single(db_id, active_table_idxs, active_col_idxs)
    
    def get_sample_hetero_data_batch(self, questions: List[str], db_ids: List[str]) -> List[HeteroData]:
        """
        æ‰¹é‡æ„å»º HeteroData å¯¹è±¡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            db_ids: æ•°æ®åº“IDåˆ—è¡¨ï¼ˆä¸questionsé•¿åº¦ç›¸åŒï¼‰
            
        Returns:
            List[HeteroData]: æ¯ä¸ªé—®é¢˜å¯¹åº”çš„ HeteroData å¯¹è±¡
        """
        if len(questions) != len(db_ids):
            raise ValueError("questions and db_ids must have the same length")
        
        # æ‰¹é‡æ‰§è¡Œ Pruningï¼ˆå…±äº«é—®é¢˜ç¼–ç ï¼‰
        pruning_results = self.prune_schema_batch(questions, db_ids)
        
        # ä¸ºæ¯ä¸ªé—®é¢˜æ„å»º HeteroData
        results = []
        for question, db_id, (active_table_idxs, active_col_idxs) in zip(questions, db_ids, pruning_results):
            try:
                data = self._build_hetero_data_single(db_id, active_table_idxs, active_col_idxs)
                results.append(data)
            except Exception as e:
                print(f"âš ï¸ Warning: Failed to build HeteroData for question '{question[:50]}...' in {db_id}: {e}")
                results.append(HeteroData())  # è¿”å›ç©ºçš„ HeteroData
        
        return results
    
    def _build_hetero_data_single(self, db_id: str, active_table_idxs: List[int], active_col_idxs: List[int]) -> HeteroData:
        """
        ä¸ºå•ä¸ªé—®é¢˜æ„å»º HeteroDataï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä» get_sample_hetero_data ä¸­æå–ï¼‰
        """
        schema = self.schemas[db_id]
        data = HeteroData()
        col_names = schema['column_names']
        
        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾
        for t_idx in active_table_idxs:
            table_name = schema['table_names'][t_idx]
            curr_table_col_idxs = [i for i in active_col_idxs if col_names[i][0] == t_idx]
            
            if not curr_table_col_idxs:
                continue
            
            # ä½¿ç”¨è¿æ¥æ± è¯»å–æ•°æ®
            db_path = os.path.join(self.train_db_root, db_id, f"{db_id}.sqlite")
            conn = self.db_pool.get_connection(db_path)
            cursor = conn.cursor()
            
            try:
                cursor.execute(f"SELECT * FROM `{table_name}` LIMIT 3")
                rows = cursor.fetchall()
                cols = [description[0] for description in cursor.description]
            except Exception as e:
                print(f"âš ï¸ Warning: Failed to read from table {table_name} in {db_id}: {e}")
                rows = []
                cols = []
            finally:
                self.db_pool.return_connection(db_path, conn)
            
            num_rows = max(len(rows), 1)
            
            # æ”¶é›†åˆ—çš„ Embedding
            table_col_embs = []
            for c_idx in curr_table_col_idxs:
                emb = self.schema_embeddings[db_id]['col_embs'][c_idx]
                table_col_embs.append(emb)
            
            if not table_col_embs:
                continue
            
            col_feats = torch.stack(table_col_embs)
            col_feats = col_feats.unsqueeze(0).expand(num_rows, -1, -1)
            
            feat_dict = {
                stype.text_embedded: col_feats.float()
            }
            
            c_names = [col_names[i][1] for i in curr_table_col_idxs]
            col_names_dict = {
                stype.text_embedded: c_names
            }
            
            data[table_name].tf = MockTensorFrame(feat_dict, col_names_dict)
            data[table_name].num_nodes = num_rows
        
        # æ„å»ºè¾¹
        for src_col, dst_col in schema['foreign_keys']:
            src_t_idx = col_names[src_col][0]
            dst_t_idx = col_names[dst_col][0]
            
            if src_t_idx in active_table_idxs and dst_t_idx in active_table_idxs:
                src_name = schema['table_names'][src_t_idx]
                dst_name = schema['table_names'][dst_t_idx]
                
                if src_name in data.node_types and dst_name in data.node_types:
                    edge_index = torch.tensor([[0], [0]], dtype=torch.long)
                    data[src_name, "fkey", dst_name].edge_index = edge_index
                    data[dst_name, "rev_fkey", src_name].edge_index = edge_index
        
        return data
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼šå…³é—­æ•°æ®åº“è¿æ¥æ± """
        if hasattr(self, 'db_pool'):
            self.db_pool.close_all()

# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    # è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
    BIRD_ROOT = "/data/cuishuai/datasets/text-to-sql/BIRD-SQL" 
    # ä½¿ç”¨ä¸€ä¸ªå°æ¨¡å‹åšæµ‹è¯•ï¼Œå®é™…è¿è¡Œæ—¶æ¢æˆ DeepSeek
    TEST_MODEL = "sentence-transformers/all-MiniLM-L6-v2" 
    
    if os.path.exists(BIRD_ROOT):
        adapter = BirdSQLAdapter(BIRD_ROOT, TEST_MODEL)
        
        # æµ‹è¯• Pruning
        q = "How many customers are from New York?"
        # å‡è®¾å–ç¬¬ä¸€ä¸ªæ•°æ®åº“
        db_id = list(adapter.schemas.keys())[0]
        
        print(f"\nğŸ” Testing Pruning on DB: {db_id}")
        t_idxs, c_idxs = adapter.prune_schema(q, db_id)
        print(f"   Selected Tables: {[adapter.schemas[db_id]['table_names'][i] for i in t_idxs]}")
        
        # æµ‹è¯• HeteroData æ„å»º
        print(f"\nğŸ“¦ Building HeteroData...")
        data = adapter.get_sample_hetero_data(q, db_id)
        print(f"   Node Types: {data.node_types}")
        print(f"   Edge Types: {data.edge_types}")
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        for nt in data.node_types:
            if hasattr(data[nt], 'tf'):
                print(f"   Table {nt}: {data[nt].tf.feat_dict[stype.text_embedded].shape}")
    else:
        print(f"âŒ Path not found: {BIRD_ROOT}")